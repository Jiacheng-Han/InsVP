{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "381c1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import VisionTransformer, PatchEmbed\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5988f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePrompt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n = 16\n",
    "        self.h = 16\n",
    "        self.net1 = nn.Sequential(\n",
    "            nn.Linear(3 * self.n * self.n, self.h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.h, 3 * self.n * self.n)\n",
    "        )\n",
    "        self.dropout1 = torch.nn.Dropout(0.1)\n",
    "        self.dropout2 = torch.nn.Dropout(0.1)\n",
    "        self.net2 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 3, 3, stride=1, padding=1)\n",
    "        )\n",
    "        self.w = nn.Parameter(torch.tensor(0.7))\n",
    "\n",
    "    def get_local_prompts(self, x):\n",
    "            # [64, 3, 224, 224]\n",
    "        B = x.shape[0]\n",
    "        n_patch = int(224 / self.n)\n",
    "        x = x.reshape(B, 3, n_patch, self.n, n_patch, self.n) # [64, 3, 14, 16, 14, 16]\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5) # [64, 14, 14, 3, 16, 16]\n",
    "        x = x.reshape(B, n_patch * n_patch, 3 * self.n * self.n)\n",
    "        x = x.reshape(B * n_patch * n_patch, 3 * self.n * self.n)\n",
    "        x = self.net1(x)\n",
    "        x = x.reshape(B, n_patch, n_patch, 3, self.n, self.n)\n",
    "        x = x.permute(0, 3, 1, 4, 2, 5) # [64, 3, 14, 16, 14, 16]\n",
    "        x = x.reshape(B, 3, 224, 224)\n",
    "        return self.dropout1(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        prompt1 = self.get_local_prompts(x)\n",
    "        prompt2 = self.dropout2(self.net2(x))\n",
    "        w = torch.sigmoid(self.w) # 保证在（0，1）\n",
    "        return (1 - w) * prompt1 + w * prompt2 + x\n",
    "    \n",
    "# model = Local_prompt().to(device)\n",
    "# random_noise = torch.randn(8, 3, 224, 224)\n",
    "# random_noise = random_noise.to(device)\n",
    "\n",
    "# # 前向传播测试\n",
    "# with torch.no_grad():\n",
    "#     output = model(random_noise)\n",
    "\n",
    "# # 输出结果\n",
    "# print(\"Random Noise Input Shape:\", random_noise.shape)\n",
    "# print(\"Model Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c1d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPrompt(nn.Module):\n",
    "    def __init__(self, prompt_num = 9):\n",
    "        super().__init__()\n",
    "        self.p = prompt_num\n",
    "        self.conv1 = nn.Conv2d(3, self.p, kernel_size=7, padding=3)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.pool1 = nn.MaxPool2d(4, 4)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.conv2 = nn.Conv2d(self.p, 3 * self.p, kernel_size = 9, padding = 4)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.pool2 = nn.MaxPool2d(3, 3)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.conv3 = nn.Conv2d(3 * self.p, 3 * self.p, kernel_size = 3, padding = 1)\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) \n",
    "        x = self.relu1(x)\n",
    "        x = x[:,:,8:216,8:216]\n",
    "        x = self.pool1(x) # [B, 9, 56, 56]\n",
    "        x = self.dropout1(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = self.conv2(x) \n",
    "        x = self.relu2(x)\n",
    "        x = x[:,:,2:50,2:50]\n",
    "        x = self.pool2(x) # [B, 27, 16, 16]\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.conv3(x) \n",
    "        x = x.reshape(-1, self.p, 768)\n",
    "#         print(x.shape)\n",
    "        return x\n",
    "\n",
    "# model = TokenPrompt().to(device)\n",
    "# random_noise = torch.randn(8, 3, 224, 224)\n",
    "# random_noise = random_noise.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model(random_noise)\n",
    "\n",
    "# # 输出结果\n",
    "# print(\"Random Noise Input Shape:\", random_noise.shape)\n",
    "# print(\"Model Output Shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ba1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsVP(VisionTransformer):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_ch=3, num_classes=120, embed_dim=768,\n",
    "                 depth=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n",
    "                 embed_layer=PatchEmbed, norm_layer=None, act_layer=None, prompt_num=9, state_dict=None, num_heads=12):\n",
    "\n",
    "        super().__init__(img_size=image_size, patch_size=patch_size, in_chans=in_ch, num_classes=num_classes,\n",
    "                         embed_dim=embed_dim, depth=depth, num_heads=num_heads, mlp_ratio=mlp_ratio,\n",
    "                         qkv_bias=qkv_bias, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n",
    "                         drop_path_rate=drop_path_rate, embed_layer=embed_layer,\n",
    "                         norm_layer=norm_layer, act_layer=act_layer)\n",
    "        \n",
    "        self.prompt_num = prompt_num\n",
    "        self.depth = depth\n",
    "        self.insprompt = nn.Parameter(torch.zeros(self.depth, self.prompt_num, embed_dim))\n",
    "        self.head = nn.Linear(self.embed_dim, self.num_classes)\n",
    "        if state_dict is not None:\n",
    "            self.load_state_dict(state_dict, strict=False)\n",
    "        self.get_image_prompt = ImagePrompt()\n",
    "        self.get_token_prompt = TokenPrompt(prompt_num = self.prompt_num)\n",
    "        self.w = nn.Parameter(torch.tensor(0.5))\n",
    "\n",
    "    def Freeze(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.insprompt.requires_grad = True\n",
    "        self.w.requires_grad = True\n",
    "        for param in self.head.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.get_image_prompt.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.get_token_prompt.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        token_prompt = self.get_token_prompt(x)\n",
    "        x = self.get_image_prompt(x)\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "        w = torch.sigmoid(self.w)\n",
    "        \n",
    "        for i in range(self.depth):\n",
    "            prompt = w * self.insprompt[i].unsqueeze(0).expand(x.shape[0], -1, -1) + (1- w) * token_prompt\n",
    "            x = torch.cat((x[:, :1, :], prompt, x[:, (1 + self.prompt_num):, :]), dim=1)\n",
    "            num_tokens = x.shape[1]\n",
    "            x = self.blocks[i](x)\n",
    "#             print(x.shape)\n",
    "            x = x[:, :num_tokens - self.prompt_num]\n",
    "#             print(x.shape)\n",
    "        \n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc_norm(x[:, 0, :])\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "# model = InsVP(num_classes=102).to(device)\n",
    "# random_noise = torch.randn(1, 3, 224, 224)\n",
    "# random_noise = random_noise.to(device)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = model(random_noise)\n",
    "#     predicted_class = torch.argmax(output, dim=1).item()\n",
    "\n",
    "# print(output.shape)\n",
    "# print(predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd19d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3785/2582952732.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(pretrained_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = \"vit_base_p16_224_in22k.pth\"\n",
    "state_dict = torch.load(pretrained_path, map_location=device)\n",
    "# 移除预训练权重中的 head 层参数\n",
    "state_dict.pop(\"head.weight\", None)\n",
    "state_dict.pop(\"head.bias\", None)\n",
    "model = InsVP(num_classes=102, state_dict = state_dict).to(device)\n",
    "model.Freeze()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f789b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomCrop(224),\n",
    "                transforms.RandomHorizontalFlip(0.5),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(256),\n",
    "                transforms.RandomCrop(224),\n",
    "#                 transforms.RandomHorizontalFlip(0.5),\n",
    "                transforms.ToTensor(),\n",
    "                normalize,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='flower102//prepare_pic//train', transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(root='flower102//prepare_pic//test', transform=transform_test)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "036c757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 4.6247, Accuracy: 0.98%\n",
      "Epoch [2/100], Loss: 4.6094, Accuracy: 1.47%\n",
      "Epoch [3/100], Loss: 4.5966, Accuracy: 2.84%\n",
      "Epoch [4/100], Loss: 4.5835, Accuracy: 4.02%\n",
      "Epoch [5/100], Loss: 4.5635, Accuracy: 6.67%\n",
      "Epoch [6/100], Loss: 4.5429, Accuracy: 11.57%\n",
      "Epoch [7/100], Loss: 4.5157, Accuracy: 15.98%\n",
      "Epoch [8/100], Loss: 4.4819, Accuracy: 20.00%\n",
      "Epoch [9/100], Loss: 4.4264, Accuracy: 26.76%\n",
      "Epoch [10/100], Loss: 4.3388, Accuracy: 36.57%\n",
      "Epoch [11/100], Loss: 4.2470, Accuracy: 49.51%\n",
      "Epoch [12/100], Loss: 4.1509, Accuracy: 57.16%\n",
      "Epoch [13/100], Loss: 4.0541, Accuracy: 66.67%\n",
      "Epoch [14/100], Loss: 3.9482, Accuracy: 70.49%\n",
      "Epoch [15/100], Loss: 3.8510, Accuracy: 76.18%\n",
      "Epoch [16/100], Loss: 3.7462, Accuracy: 79.80%\n",
      "Epoch [17/100], Loss: 3.6393, Accuracy: 82.75%\n",
      "Epoch [18/100], Loss: 3.5330, Accuracy: 85.78%\n",
      "Epoch [19/100], Loss: 3.4330, Accuracy: 87.25%\n",
      "Epoch [20/100], Loss: 3.3162, Accuracy: 88.92%\n",
      "Epoch [21/100], Loss: 3.2064, Accuracy: 90.39%\n",
      "Epoch [22/100], Loss: 3.1019, Accuracy: 91.08%\n",
      "Epoch [23/100], Loss: 2.9891, Accuracy: 91.76%\n",
      "Epoch [24/100], Loss: 2.8809, Accuracy: 92.94%\n",
      "Epoch [25/100], Loss: 2.7759, Accuracy: 93.92%\n",
      "Epoch [26/100], Loss: 2.6682, Accuracy: 94.02%\n",
      "Epoch [27/100], Loss: 2.5623, Accuracy: 95.29%\n",
      "Epoch [28/100], Loss: 2.4600, Accuracy: 95.98%\n",
      "Epoch [29/100], Loss: 2.3558, Accuracy: 96.47%\n",
      "Epoch [30/100], Loss: 2.2582, Accuracy: 97.06%\n",
      "Epoch [31/100], Loss: 2.1659, Accuracy: 97.25%\n",
      "Epoch [32/100], Loss: 2.0739, Accuracy: 97.55%\n",
      "Epoch [33/100], Loss: 1.9833, Accuracy: 98.24%\n",
      "Epoch [34/100], Loss: 1.8952, Accuracy: 97.94%\n",
      "Epoch [35/100], Loss: 1.8105, Accuracy: 98.04%\n",
      "Epoch [36/100], Loss: 1.7330, Accuracy: 98.53%\n",
      "Epoch [37/100], Loss: 1.6520, Accuracy: 98.43%\n",
      "Epoch [38/100], Loss: 1.5815, Accuracy: 98.73%\n",
      "Epoch [39/100], Loss: 1.5069, Accuracy: 98.82%\n",
      "Epoch [40/100], Loss: 1.4352, Accuracy: 99.02%\n",
      "Epoch [41/100], Loss: 1.3651, Accuracy: 99.51%\n",
      "Epoch [42/100], Loss: 1.3022, Accuracy: 99.51%\n",
      "Epoch [43/100], Loss: 1.2391, Accuracy: 99.51%\n",
      "Epoch [44/100], Loss: 1.1857, Accuracy: 99.22%\n",
      "Epoch [45/100], Loss: 1.1273, Accuracy: 99.80%\n",
      "Epoch [46/100], Loss: 1.0713, Accuracy: 99.71%\n",
      "Epoch [47/100], Loss: 1.0213, Accuracy: 99.71%\n",
      "Epoch [48/100], Loss: 0.9742, Accuracy: 99.90%\n",
      "Epoch [49/100], Loss: 0.9267, Accuracy: 100.00%\n",
      "Epoch [50/100], Loss: 0.8840, Accuracy: 99.71%\n",
      "Epoch [51/100], Loss: 0.8424, Accuracy: 99.90%\n",
      "Epoch [52/100], Loss: 0.8026, Accuracy: 100.00%\n",
      "Epoch [53/100], Loss: 0.7623, Accuracy: 100.00%\n",
      "Epoch [54/100], Loss: 0.7294, Accuracy: 99.90%\n",
      "Epoch [55/100], Loss: 0.6902, Accuracy: 99.90%\n",
      "Epoch [56/100], Loss: 0.6643, Accuracy: 100.00%\n",
      "Epoch [57/100], Loss: 0.6299, Accuracy: 100.00%\n",
      "Epoch [58/100], Loss: 0.6023, Accuracy: 100.00%\n",
      "Epoch [59/100], Loss: 0.5754, Accuracy: 99.90%\n",
      "Epoch [60/100], Loss: 0.5516, Accuracy: 100.00%\n",
      "Epoch [61/100], Loss: 0.5279, Accuracy: 100.00%\n",
      "Epoch [62/100], Loss: 0.5011, Accuracy: 100.00%\n",
      "Epoch [63/100], Loss: 0.4774, Accuracy: 100.00%\n",
      "Epoch [64/100], Loss: 0.4559, Accuracy: 100.00%\n",
      "Epoch [65/100], Loss: 0.4397, Accuracy: 100.00%\n",
      "Epoch [66/100], Loss: 0.4214, Accuracy: 100.00%\n",
      "Epoch [67/100], Loss: 0.4044, Accuracy: 100.00%\n",
      "Epoch [68/100], Loss: 0.3853, Accuracy: 100.00%\n",
      "Epoch [69/100], Loss: 0.3695, Accuracy: 100.00%\n",
      "Epoch [70/100], Loss: 0.3541, Accuracy: 100.00%\n",
      "Epoch [71/100], Loss: 0.3422, Accuracy: 100.00%\n",
      "Epoch [72/100], Loss: 0.3283, Accuracy: 100.00%\n",
      "Epoch [73/100], Loss: 0.3172, Accuracy: 100.00%\n",
      "Epoch [74/100], Loss: 0.3045, Accuracy: 100.00%\n",
      "Epoch [75/100], Loss: 0.2914, Accuracy: 100.00%\n",
      "Epoch [76/100], Loss: 0.2803, Accuracy: 100.00%\n",
      "Epoch [77/100], Loss: 0.2705, Accuracy: 100.00%\n",
      "Epoch [78/100], Loss: 0.2629, Accuracy: 100.00%\n",
      "Epoch [79/100], Loss: 0.2540, Accuracy: 100.00%\n",
      "Epoch [80/100], Loss: 0.2422, Accuracy: 100.00%\n",
      "Epoch [81/100], Loss: 0.2364, Accuracy: 100.00%\n",
      "Epoch [82/100], Loss: 0.2260, Accuracy: 100.00%\n",
      "Epoch [83/100], Loss: 0.2200, Accuracy: 100.00%\n",
      "Epoch [84/100], Loss: 0.2130, Accuracy: 100.00%\n",
      "Epoch [85/100], Loss: 0.2076, Accuracy: 100.00%\n",
      "Epoch [86/100], Loss: 0.2014, Accuracy: 100.00%\n",
      "Epoch [87/100], Loss: 0.1938, Accuracy: 100.00%\n",
      "Epoch [88/100], Loss: 0.1880, Accuracy: 100.00%\n",
      "Epoch [89/100], Loss: 0.1806, Accuracy: 100.00%\n",
      "Epoch [90/100], Loss: 0.1767, Accuracy: 100.00%\n",
      "Epoch [91/100], Loss: 0.1708, Accuracy: 100.00%\n",
      "Epoch [92/100], Loss: 0.1651, Accuracy: 100.00%\n",
      "Epoch [93/100], Loss: 0.1613, Accuracy: 100.00%\n",
      "Epoch [94/100], Loss: 0.1568, Accuracy: 100.00%\n",
      "Epoch [95/100], Loss: 0.1517, Accuracy: 100.00%\n",
      "Epoch [96/100], Loss: 0.1474, Accuracy: 100.00%\n",
      "Epoch [97/100], Loss: 0.1432, Accuracy: 100.00%\n",
      "Epoch [98/100], Loss: 0.1398, Accuracy: 100.00%\n",
      "Epoch [99/100], Loss: 0.1364, Accuracy: 100.00%\n",
      "Epoch [100/100], Loss: 0.1323, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练循环\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# 保存训练后的模型\n",
    "torch.save(model.state_dict(), \"InsVP.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "328b36c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.81%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "# 计算测试集上的平均损失和准确率\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5918bde-89ed-4cb8-8c14-792faeced1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
